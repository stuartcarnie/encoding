// Generated by PeachPy 0.2.0 from unpack.py


// func unpack240SSE(v uint64, dst_base uintptr, dst_len uint, dst_cap uint)
TEXT ·unpack240SSE(SB),4,$0-32
	MOVQ dst_base+8(FP), AX
	MOVQ dst_len+16(FP), BX
	SHRQ $3, BX
	MOVQ $1, CX
	MOVQ CX, X0
	MOVQ $1, CX
	PINSRQ $1, CX, X0
loop_begin:
		MOVOU X0, 0(AX)
		MOVOU X0, 16(AX)
		MOVOU X0, 32(AX)
		MOVOU X0, 48(AX)
		ADDQ $64, AX
		SUBQ $1, BX
		JNE loop_begin
	RET

// func unpack240AVX2(v uint64, dst_base uintptr, dst_len uint, dst_cap uint)
TEXT ·unpack240AVX2(SB),4,$0-32
	MOVQ dst_base+8(FP), AX
	MOVQ dst_len+16(FP), BX
	SHRQ $3, BX
	MOVQ $1, CX
	MOVQ CX, X0
	BYTE $0xC4; BYTE $0xE2; BYTE $0x7D; BYTE $0x59; BYTE $0xC0 // VPBROADCASTQ ymm0, xmm0
loop_begin:
		BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x00 // VMOVDQU [rax], ymm0
		BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x40; BYTE $0x20 // VMOVDQU [rax + 32], ymm0
		ADDQ $64, AX
		SUBQ $1, BX
		JNE loop_begin
	BYTE $0xC5; BYTE $0xF8; BYTE $0x77 // VZEROUPPER
	RET

// func unpack60AVX2(v uint64, dst_base uintptr, dst_len uint, dst_cap uint)
TEXT ·unpack60AVX2(SB),4,$0-32
	MOVQ v+0(FP), AX
	MOVQ dst_base+8(FP), BX
	MOVQ AX, X0
	SHRQ $1, AX
	PINSRQ $1, AX, X0
	SHRQ $1, AX
	MOVQ AX, X1
	SHRQ $1, AX
	PINSRQ $1, AX, X1
	MOVQ $1, AX
	MOVQ AX, X2
	BYTE $0xC4; BYTE $0xE2; BYTE $0x7D; BYTE $0x59; BYTE $0xD2 // VPBROADCASTQ ymm2, xmm2
	BYTE $0xC4; BYTE $0xE3; BYTE $0x65; BYTE $0x38; BYTE $0xD8; BYTE $0x00 // VINSERTI128 ymm3, ymm3, xmm0, 0
	BYTE $0xC4; BYTE $0xE3; BYTE $0x65; BYTE $0x38; BYTE $0xD9; BYTE $0x01 // VINSERTI128 ymm3, ymm3, xmm1, 1
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x60 // VMOVDQU [rbx + 96], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	ADDQ $128, BX
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x60 // VMOVDQU [rbx + 96], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	ADDQ $128, BX
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x60 // VMOVDQU [rbx + 96], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	ADDQ $128, BX
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x04 // VPSRLQ ymm3, ymm0, 4
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x04 // VPSRLQ ymm0, ymm3, 4
	BYTE $0xC5; BYTE $0xF8; BYTE $0x77 // VZEROUPPER
	RET

// func unpack30AVX2(v uint64, dst_base uintptr, dst_len uint, dst_cap uint)
TEXT ·unpack30AVX2(SB),4,$0-32
	MOVQ v+0(FP), AX
	MOVQ dst_base+8(FP), BX
	MOVQ AX, X0
	SHRQ $2, AX
	PINSRQ $1, AX, X0
	SHRQ $2, AX
	MOVQ AX, X1
	SHRQ $2, AX
	PINSRQ $1, AX, X1
	MOVQ $3, AX
	MOVQ AX, X2
	MOVQ $9223372036854775808, AX
	PXOR X3, X3
	MOVQ AX, X3
	PINSRQ $1, AX, X3
	BYTE $0xC4; BYTE $0xE3; BYTE $0x5D; BYTE $0x38; BYTE $0xE3; BYTE $0x00 // VINSERTI128 ymm4, ymm4, xmm3, 0
	BYTE $0xC4; BYTE $0xE2; BYTE $0x7D; BYTE $0x59; BYTE $0xD2 // VPBROADCASTQ ymm2, xmm2
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x38; BYTE $0xE8; BYTE $0x00 // VINSERTI128 ymm5, ymm5, xmm0, 0
	BYTE $0xC4; BYTE $0xE3; BYTE $0x55; BYTE $0x38; BYTE $0xE9; BYTE $0x01 // VINSERTI128 ymm5, ymm5, xmm1, 1
	BYTE $0xC5; BYTE $0xD5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm5, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD5; BYTE $0x08 // VPSRLQ ymm0, ymm5, 8
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xEA // VPAND ymm5, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x6B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x73; BYTE $0xD0; BYTE $0x08 // VPSRLQ ymm5, ymm0, 8
	BYTE $0xC5; BYTE $0xD5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm5, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD5; BYTE $0x08 // VPSRLQ ymm0, ymm5, 8
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xEA // VPAND ymm5, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x6B; BYTE $0x60 // VMOVDQU [rbx + 96], ymm5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x73; BYTE $0xD0; BYTE $0x08 // VPSRLQ ymm5, ymm0, 8
	ADDQ $128, BX
	BYTE $0xC5; BYTE $0xD5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm5, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD5; BYTE $0x08 // VPSRLQ ymm0, ymm5, 8
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xEA // VPAND ymm5, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x6B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm5
	BYTE $0xC5; BYTE $0xD5; BYTE $0x73; BYTE $0xD0; BYTE $0x08 // VPSRLQ ymm5, ymm0, 8
	BYTE $0xC5; BYTE $0xD5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm5, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD5; BYTE $0x08 // VPSRLQ ymm0, ymm5, 8
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xEA // VPAND ymm5, ymm0, ymm2
	BYTE $0xC4; BYTE $0xE2; BYTE $0xDD; BYTE $0x8E; BYTE $0x6B; BYTE $0x60 // VPMASKMOVQ [rbx + 96], ymm4, ymm5
	BYTE $0xC5; BYTE $0xF8; BYTE $0x77 // VZEROUPPER
	RET

// func unpack20AVX2(v uint64, dst_base uintptr, dst_len uint, dst_cap uint)
TEXT ·unpack20AVX2(SB),4,$0-32
	MOVQ v+0(FP), AX
	MOVQ dst_base+8(FP), BX
	MOVQ AX, X0
	SHRQ $3, AX
	PINSRQ $1, AX, X0
	SHRQ $3, AX
	MOVQ AX, X1
	SHRQ $3, AX
	PINSRQ $1, AX, X1
	MOVQ $7, AX
	MOVQ AX, X2
	BYTE $0xC4; BYTE $0xE2; BYTE $0x7D; BYTE $0x59; BYTE $0xD2 // VPBROADCASTQ ymm2, xmm2
	BYTE $0xC4; BYTE $0xE3; BYTE $0x65; BYTE $0x38; BYTE $0xD8; BYTE $0x00 // VINSERTI128 ymm3, ymm3, xmm0, 0
	BYTE $0xC4; BYTE $0xE3; BYTE $0x65; BYTE $0x38; BYTE $0xD9; BYTE $0x01 // VINSERTI128 ymm3, ymm3, xmm1, 1
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x0C // VPSRLQ ymm0, ymm3, 12
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x20 // VMOVDQU [rbx + 32], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x0C // VPSRLQ ymm3, ymm0, 12
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x43; BYTE $0x40 // VMOVDQU [rbx + 64], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x0C // VPSRLQ ymm0, ymm3, 12
	BYTE $0xC5; BYTE $0xFD; BYTE $0xDB; BYTE $0xDA // VPAND ymm3, ymm0, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x5B; BYTE $0x60 // VMOVDQU [rbx + 96], ymm3
	BYTE $0xC5; BYTE $0xE5; BYTE $0x73; BYTE $0xD0; BYTE $0x0C // VPSRLQ ymm3, ymm0, 12
	ADDQ $128, BX
	BYTE $0xC5; BYTE $0xE5; BYTE $0xDB; BYTE $0xC2 // VPAND ymm0, ymm3, ymm2
	BYTE $0xC5; BYTE $0xFE; BYTE $0x7F; BYTE $0x03 // VMOVDQU [rbx], ymm0
	BYTE $0xC5; BYTE $0xFD; BYTE $0x73; BYTE $0xD3; BYTE $0x0C // VPSRLQ ymm0, ymm3, 12
	BYTE $0xC5; BYTE $0xF8; BYTE $0x77 // VZEROUPPER
	RET
